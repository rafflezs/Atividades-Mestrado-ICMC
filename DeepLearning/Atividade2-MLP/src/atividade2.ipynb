{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from mlp import MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## O que é uma Rede Neural MLP (Multi-Layer Perceptron)?\n",
    "\n",
    "Trata-se de um tipo de rede neural artificial composta por várias camadas de neurônios. Cada neurônio em uma camada está conectado a todos os neurônios da camada seguinte. A rede MLP é composta basicamente pelas seguintes camadas:\n",
    "\n",
    "- **Camada de entrada**: onde os dados entram na rede;\n",
    "- **Camada(s) oculta(s)**: onde as conexões e transformações dos dados ocorrem através dos pesos e funções de ativação;\n",
    "- **Camada de saída**: onde a previsão ou classificação é realizada;\n",
    "\n",
    "A MLP é chamada de *perceptron multicamadas* porque possui uma ou mais camadas ocultas entre a entrada e a saída. Esse tipo de rede é capaz de resolver problemas não-linearmente separáveis, como o problema XOR.\n",
    "\n",
    "### Algoritmo de Backpropagation\n",
    "\n",
    "O backpropagation é o algoritmo comumente utilizado para treinar uma rede MLP, que funciona ajustando os pesos da rede de modo a minimizar o erro entre as previsões da rede e os valores reais. O processo de treinamento envolve duas fases principais:\n",
    "\n",
    "1. **Forward Pass**: Os dados de entrada passam pela rede e produzem uma saída;\n",
    "2. **Backward Pass**: O erro (diferença entre a saída prevista e a saída real) é propagado de volta pela rede, ajustando os pesos em cada camada para reduzir o erro na próxima iteração.\n",
    "\n",
    "O ajuste dos pesos é feito utilizando o método do gradiente descendente, que atualiza os pesos proporcionalmente à derivada do erro em relação aos pesos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Problema XOR\n",
    "\n",
    "O problema XOR é um exemplo clássico de problema que não pode ser resolvido por uma rede Perceptron simples, pois não é linearmente separável. Dessa forma, o MLP entra como uma forma de solucionar esse cenário.\n",
    "\n",
    "### Definição\n",
    "\n",
    "A função XOR é definida da seguinte forma:\n",
    "\n",
    "- \\( 0 ⊕ 0 = 0 \\)\n",
    "- \\( 0 ⊕ 1 = 1 \\)\n",
    "- \\( 1 ⊕ 0 = 1 \\)\n",
    "- \\( 1 ⊕ 1 = 0 \\)\n",
    "\n",
    "Neste notebook, foi construída e treinada uma rede MLP para aprender a função XOR.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#XOR\n",
    "X = np.array(\n",
    "    [\n",
    "        [0, 0], \n",
    "        [0, 1], \n",
    "        [1, 0], \n",
    "        [1, 1]\n",
    "    ])\n",
    "\n",
    "y = np.array([\n",
    "        [0], # 0 xor 0 = 0\n",
    "        [1], # 0 xor 1 = 1\n",
    "        [1], # 1 xor 0 = 1\n",
    "        [0]  # 1 xor 1 = 0\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Dimensões de X_train: {X.shape}\")\n",
    "print(f\"Dimensões de y_train: {y.shape}\")\n",
    "mlp = MLP(n_inputs=X.shape[1], n_hidden=X.shape[1], n_output=1)\n",
    "mlp.treino(X, y, epochs=10000, eta=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(len(X)):\n",
    "    print(f'Input: {X[_]} | Output: {int(mlp.predict(X[_]) > 0.5)} | Expected: {y[_]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teste com mais de 2 entradas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper.load_data import load_data\n",
    "import os\n",
    "data_dir = os.path.join(os.getcwd(), '..', 'data', 'XOR')\n",
    "\n",
    "for file in os.listdir(data_dir):\n",
    "    print(f\"Arquivo: {file}\")\n",
    "    file_path = os.path.join(data_dir, file)\n",
    "    X, y = load_data(file_path=file_path)\n",
    "\n",
    "    mlp = MLP(n_inputs=X.shape[1], n_hidden=X.shape[1], n_output=1)\n",
    "    mlp.treino(X, y, epochs=10000, eta=0.1)\n",
    "\n",
    "    print(f'Input: {X[_]} | Output: {int(mlp.predict(X[_]) > 0.5)} | Expected: {y[_]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problema de Auto-associação (Autoencoder)\n",
    "\n",
    "O problema de auto-associação envolve treinar uma rede neural para mapear padrões de entrada em padrões de saída idênticos. Um exemplo clássico é utilizar matrizes de identidade como entrada e saída. A rede é composta de uma camada oculta com menos neurônios do que a camada de entrada/saída, o que obriga a rede a aprender uma codificação comprimida dos dados.\n",
    "\n",
    "### Estrutura da Rede\n",
    "\n",
    "- Para o caso de \\(8x8\\), usou-se 8 neurônios na camada de entrada, 3 neurônios na camada oculta e 8 neurônios na camada de saída (pois log2(8)=3).\n",
    "- Para o caso de \\(15x15\\), usou-se 15 neurônios na entrada, 4 neurônios na camada oculta e 15 neurônios na saída (pois log2(15)≈4).\n",
    "\n",
    "O objetivo é que a rede aprenda a realizar o mapeamento identidade, comprimindo os dados na camada oculta e depois decodificando-os para restaurar a saída original.\n",
    "\n",
    "### Algoritmo\n",
    "\n",
    "Utilizamos o algoritmo de backpropagation para ajustar os pesos da rede de modo a minimizar o erro entre a saída prevista e a saída real (matriz identidade).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caso_encoder = [\n",
    "    {'N': 8, 'epochs': 10000},\n",
    "    {'N': 15, 'epochs': 15000},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for caso in caso_encoder:\n",
    "    N = caso['N']\n",
    "    epochs = caso['epochs']\n",
    "    \n",
    "    # Identidade NxN\n",
    "    X = np.identity(N)\n",
    "    y = X.copy()\n",
    "\n",
    "    # Camada escondida Log2(N)\n",
    "    hidden_size = int(np.log2(N))\n",
    "    \n",
    "    # Teinar a rede para o encoder de tamanho N\n",
    "    mlp = MLP(n_inputs=N, n_hidden=hidden_size, n_output=N)\n",
    "    mlp.treino(y=X, X=y, epochs=epochs, eta=0.1)\n",
    "    \n",
    "    # 'Predict'\n",
    "    predictions = mlp.predict(X)\n",
    "    print(f\"Predict: Encoder de tamanho N={N}:\\n\", np.round(predictions, decimals=2))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
