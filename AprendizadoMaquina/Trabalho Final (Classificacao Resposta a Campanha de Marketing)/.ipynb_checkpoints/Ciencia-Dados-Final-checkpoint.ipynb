{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# INTRODUÇÃO A CIENCIA DE DADOS (SCC5848)\n",
    "\n",
    "### **Trabalho Final** - Análise do _dataset_ [A Symphony of Customer Interactions](https://www.kaggle.com/datasets/youssefismail20/a-symphony-of-customer-interactions)\n",
    "\n",
    "### Profª. Roseli Ap. Francelin Romero\n",
    "\n",
    "### Alunos:\n",
    "\n",
    "- __Julyana Flores de Prá__ (NUSP: 15600911)\n",
    "\n",
    "- __Thiago Rafael Mariotti Claudio__ (NUSP: 15611674)\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "A base de dados [A Symphony of Customer Interactions](https://www.kaggle.com/datasets/youssefismail20/a-symphony-of-customer-interactions) apresenta um conjunto de interações e aquisições em um _e-commerce_ através de diferentes atributos, como dados do comprador, ocupação, recepção à campanhas de _marketing_ e operações de compra, permitindo um estudo sobre o comportamento humano e suas interações financeiras.  \n",
    "Ao longo da análise esperamos responder algumas hipóteses.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## 1. Inicialização da base de dados\n",
    "\n",
    "Nesta seção executaremos operações básicas para carregar, explorar e validar o conjunto de dados, a fim de encontrar valores desconhecidos ou não numéricos, verificar a tipagem dos dados e, caso necessário, efetuar correções com o intuito de preparar o _dataset_ para as fases de exploração e análise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'A Symphony of Customer Interactions.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m initial_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mA Symphony of Customer Interactions.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'A Symphony of Customer Interactions.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "initial_df = pd.read_csv(\"A Symphony of Customer Interactions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display(initial_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display(initial_df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Como observado acima, não foram encontrados valores desconhecidos em todas entradas do conjunto, em decorrência do _score_ de usabilidade do _dataset_ no _Kaggle_.  \n",
    "Verifica-se no entanto que alguns atributos apresentados como listas de valores numéricos são dados como objetos, possivelmente uma cadeia unitária de caracteres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import ast \n",
    "is_it_string_question_mark = initial_df['Purchase_Amounts'][1]\n",
    "\n",
    "if isinstance(is_it_string_question_mark, str):\n",
    "    print(\"O valor é uma string\")\n",
    "    print(is_it_string_question_mark)\n",
    "    print(is_it_string_question_mark[0])\n",
    "    print(type(is_it_string_question_mark[0]))\n",
    "else:\n",
    "    print(\"O valor é uma lista de reais\")\n",
    "\n",
    "print(\"======LINEBREAK======\")\n",
    "    \n",
    "float_list = ast.literal_eval(is_it_string_question_mark)\n",
    "print(float_list) #lista de purchase amounts\n",
    "print(float_list[0])\n",
    "print(\"O valor agora é uma lista de reais.\", type(float_list[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Na operação acima, podemos notar que os valores da coluna formatadas como listas são na verdade _strings_  (__'['not','a','list']'__ $\\neq$ __['yes','a','list']__, onde a primeira estrutura é uma string com valor __'['not','a','list']'__ e a segunda é uma lista/array de tamanho 3, contendo os valores __yes__,__a__,__list__).  \n",
    "Podemos realizar a transformação dessas colunas conforme a operação abaixo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for col in ['Products','Purchase_Amounts','Hashtags_Used','Price_Changes_Over_Time']:\n",
    "    initial_df[col] = initial_df[col].apply(ast.literal_eval) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Valores totais em Customer_ID: {initial_df['Customer_ID'].count()}\\nValores unicos: {initial_df['Customer_ID'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Verifica-se que todas as operações contabilizadas nessa base de dados são de clientes diferentes, uma vez que a contagem de valores é igual para a coluna 'Customer_ID', independente do método de contagem (total ou único).  \n",
    "Podemos assumir então que a base apresenta a coletânea de compras para clientes distintos, que aparecem UMA ÚNICA VEZ. Dessa maneira podemos realizar algumas transformações nos dados, como a contagem de produtos comprados, através da contagem de itens na lista 'Products'.\n",
    "\n",
    "<br><br>\n",
    "\n",
    "Além disso, podemos gerar algumas colunas de atributos importantes, para facilitar análises futuras.  \n",
    "Nesse caso vamos gerar o __Total de Produtos Comprados__ pelo cliente e o __Total Gasto__ pelo cliente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "initial_df['Products_Bought'] = initial_df['Products'].apply(len)\n",
    "\n",
    "display(initial_df[['Name','Products','Products_Bought']])\n",
    "display(initial_df['Products_Bought'].describe()) # Todos os clientes compraram EXATAMENTE 5 produtos cada. POR QUE????????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "initial_df['Total_Spent_by_Client'] = initial_df['Purchase_Amounts'].apply(sum)\n",
    "\n",
    "display(initial_df[['Name','Purchase_Amounts','Total_Spent_by_Client', 'Products_Bought']])\n",
    "display(initial_df['Total_Spent_by_Client'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "initial_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# tbm dá pra tentar fazer assim\n",
    "# Assim é o unico jeito de fazer, o jeito anterior era uma piada de mal gosto\n",
    "\n",
    "import re\n",
    "import datetime\n",
    "\n",
    "def datetime_fix(datetime_string: str):\n",
    "\n",
    "    fixed_str_list = datetime_string.replace(\"datetime.datetime\", \"datetime\").replace(\" \", \"\")\n",
    "    matches = re.findall(r'datetime\\(([^)]+)\\)', fixed_str_list)\n",
    "\n",
    "    date_list = []\n",
    "    for match in matches:\n",
    "        args = [int(arg) for arg in match.split(\",\")]\n",
    "        dt = datetime.datetime(*args)\n",
    "        date_list.append(dt)\n",
    "\n",
    "    return date_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "initial_df['Purchase_Date'] = initial_df['Purchase_Date'].apply(datetime_fix)\n",
    "\n",
    "display(initial_df['Purchase_Date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Podemos também excluir alguns atributos que não acrescem informações à nossa exploração, como __Customer_ID__ e __Name__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "drop_cols = ['Customer_ID','Name']\n",
    "initial_df = initial_df.drop(columns=drop_cols)\n",
    "# features não dropadas: 'Occupation','Location', 'Purchase_Date'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Baseado nas informações gerais da base de dados podemos formular algumas hipoteses, como:\n",
    "\n",
    "\n",
    "1. __Existe correlação entre idade e volume de compras?__  \n",
    "\n",
    "2. __Existe uma relação entre a idade do cliente e a interação com as campanhas de compra?__  \n",
    "\n",
    "3. __Existe uma relação entre a taxa de conversão (volume de compra) e as interações no _website_?__  \n",
    "\n",
    "4. __A faixa socioeconômica do cliente afeta suas interações de escambo?__  \n",
    "\n",
    "5. __A reação à campanha de _marketing_ é um bom indicativo das interações do cliente? É possível prever sua reação baseada nas interações?__  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "---\n",
    "## 2. Exploração dos dados\n",
    "\n",
    "Nesta seção estão concentradas as explorações e observações de exploração da base de dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "\n",
    "num_df = initial_df.select_dtypes(include=['number'])\n",
    "\n",
    "corr_mat_num = num_df.corr()\n",
    "\n",
    "sns.heatmap(corr_mat_num, annot=True, cmap='coolwarm', fmt='.2f')\n",
    "\n",
    "plt.title('Matriz de Correlação - Dados Númericos')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# 'Products_Brought' removido\n",
    "attr = ['Age', 'Total_Spent_by_Client'] # como o dataset apresenta sempre 5 produtos comprados por cliente não há como\n",
    "                                        # observar correlação entre quantidade de produtos comprados, idade e total gasto\n",
    "corr = initial_df[attr].corr()\n",
    "\n",
    "sns.heatmap(corr, annot=True, cmap='coolwarm')\n",
    "\n",
    "plt.title('Matriz de Correlação - Idade x Total Gasto')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(15, 8))\n",
    "initial_df.groupby('Age')['Total_Spent_by_Client'].sum().plot(kind='bar', color=['green','blue','red']) # Oneliner is my passion\n",
    "plt.xlabel('Age Group')\n",
    "plt.ylabel('Total Spent')\n",
    "plt.title('Total Spent by Customer Age Group')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "initial_df['Age_Group'] = pd.cut(initial_df['Age'], bins=[0, 25, 34, 60, 100], labels=['Youngling', 'Young Adult', 'Adult', 'Elder']) # definição formal das faixas etarias\n",
    "                                                                                                                          # adolescencia (juventude): 12 a 24\n",
    "                                                                                                                          # jovem adulto: 25 a 34\n",
    "                                                                                                                          # adulto: 35 a 60\n",
    "                                                                                                                          # idoso (terceira idade): 61 em diante\n",
    "\n",
    "plt.figure(figsize=(15, 8))\n",
    "initial_df.groupby('Age_Group', observed=True)['Total_Spent_by_Client'].sum().plot(kind='bar', color=['green','blue','red'], style='plain') # Oneliner is my passion\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Total Spent')\n",
    "plt.title('Total Spent by Customer Age Group')\n",
    "plt.show()\n",
    "\n",
    "#desculpa eu não consigo lidar com codigo com warning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Embora a matriz de correlação demonstre um baixa relação entre **Idade** e **Total Gasto**, é importante observar como a distribuição do volume de compras se comporta em relação a distribuição da idade do cliente.  \n",
    "Nesse caso observamos uma boa distribuição em compras por todas as idades, embora o agrupamento por faixa etária possa gerar enviesamento, mostrando uma grande concentração de volume de compras no grupo __Adulto__ (35 à 60 anos). Podemos entender melhor ao observar a distribuição de idade na base de dados.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "iqr = initial_df['Age'].quantile(0.75) - initial_df['Age'].quantile(0.25)\n",
    "std = initial_df['Age'].std()\n",
    "skew = initial_df['Age'].skew()\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 8))\n",
    "\n",
    "axes[0].hist(initial_df['Age'], bins=20, edgecolor='black')\n",
    "axes[0].set_xlabel('Age')\n",
    "axes[0].set_ylabel('Number of Customers')\n",
    "axes[0].set_title('Age Distribution')\n",
    "\n",
    "# IQR\n",
    "axes[1].boxplot([initial_df['Age']], vert=False, showfliers=False, patch_artist=True)\n",
    "axes[1].set_xlabel('Age')\n",
    "axes[1].set_title('Interquartile Range (IQR)')\n",
    "\n",
    "# STD\n",
    "axes[2].hist(initial_df['Age'] - initial_df['Age'].mean(), bins=10, edgecolor='black')\n",
    "axes[2].set_xlabel('Age from Mean')\n",
    "axes[2].set_ylabel('Number of Customers')\n",
    "axes[2].set_title('Standard Deviation (STD)')\n",
    "\n",
    "for ax in axes:\n",
    "    ax.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Nota-se uma distribuição normal para as idades dos clientes. A razão para maior volume na faixa etária __Adulto__ vêm do fato de que esse grupo engloba um número maior de clientes, e por isso dá impressão de desbalanceamento.\n",
    "\n",
    "Dito isso, não parece haver correlação entre idade e volume de compras. Todos as faixas etárias tiveram gastos semelhantes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Podemos também averiguar a interação entre faixas etárias e campanhas de compra, verificando se algum grupo responde melhor à propagandas, bem como a interação em redes sociais com o _ecommerce_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "campaign_by_age = initial_df.groupby('Age_Group', observed=True)['Response_to_Campaign'].value_counts().unstack(fill_value=0)\n",
    "display(campaign_by_age.div(campaign_by_age.sum(axis=1), axis=0) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Podemos constatar que a base de dados está bem balanceada, e portanto todos os grupos etários apresentam a mesma tendência de resposta a campanha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "avg_engagement_by_age = initial_df.groupby('Age_Group', observed=True)[['Email_Open_Rate', 'Click_Through_Rate_Marketing', 'Social_Media_Engagement']].mean()\n",
    "display(avg_engagement_by_age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(avg_engagement_by_age.index, avg_engagement_by_age['Email_Open_Rate'], label='Email Open Rate')\n",
    "plt.plot(avg_engagement_by_age.index, avg_engagement_by_age['Click_Through_Rate_Marketing'], label='Click-Through Rate')\n",
    "plt.xlabel('Age Group')\n",
    "plt.ylabel('Engagement Rate')\n",
    "plt.title('Engagement Rates by Age Group')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(avg_engagement_by_age.index, avg_engagement_by_age['Social_Media_Engagement'])\n",
    "plt.xlabel('Age Group')\n",
    "plt.ylabel('Social Media Engagement')\n",
    "plt.title('Social Media Engagement by Age Group')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Também não observamos nenhuma diferença entre as faixas etárias e suas interações às redes sociais, _e-mails_ e derivados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "social_interaction_age = initial_df.groupby('Age')[['Email_Open_Rate', 'Click_Through_Rate_Marketing', 'Social_Media_Engagement']].sum()\n",
    "display(social_interaction_age.sort_values(by=['Email_Open_Rate', 'Click_Through_Rate_Marketing', 'Social_Media_Engagement'], ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Embora tenham mais clientes na faixa de __63 anos__ com maior interação ao aspecto social do _ecommerce_ não é possível observar nenhuma tendência, novamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#h4 ?\n",
    "import plotly.express as px\n",
    "\n",
    "fig_box = px.box(initial_df, x='Income_Level', y='Total_Spent_by_Client',\n",
    "                 labels={'Income_Level': 'Income Level', 'Total_Spent_by_Client': 'Total Spent'},\n",
    "                 title='Box Plot: Total Spent by Client by Income Level',\n",
    "                 color='Income_Level')\n",
    "\n",
    "fig_box.show()\n",
    "\n",
    "#nao tem relacao de nada com porra nenhuma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#h5 ?\n",
    "fig_pie = px.pie(initial_df['Response_to_Campaign'].value_counts(), values=initial_df['Response_to_Campaign'].value_counts().values,\n",
    "                 names=initial_df['Response_to_Campaign'].value_counts().index,\n",
    "                 title='Response to Campaign Distribution')\n",
    "\n",
    "fig_pie.show()\n",
    "\n",
    "#a gente vai usar só o positivo e negativo como classificador? ou vamos usar alguma tecnica pra transformar 3 em 2 classes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "---\n",
    "## 3. Analisando a base de dados\n",
    "\n",
    "Neste seção serão efetuadas operações para análise de dados e teste de hipóteses, bem como o desenvolvimento do classificador relacionado à hipótese 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Cluster para features do marketing\n",
    "marketing_feats = initial_df[['Total_Spent_by_Client', 'Website_Visits', 'Click_Through_Rate', 'Pages_Viewed', 'Time_Spent_on_Website', 'Social_Media_Engagement', 'Followers_Count', 'Customer_Satisfaction_Score', 'Email_Open_Rate', 'Click_Through_Rate_Marketing']].copy()\n",
    "\n",
    "display(marketing_feats.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "scaler = StandardScaler()\n",
    "cluster = scaler.fit_transform(marketing_feats)\n",
    "\n",
    "kmeans = KMeans(n_clusters=3, random_state=13) #petista ou swiftie? \n",
    "                                               # Implicando que a Taylor não meteu um 13 na urna se ela é você; mais respeito please\n",
    "\n",
    "marketing_feats[\"Cluster\"] = kmeans.fit_predict(cluster) # to bobo e tava fazendo kmeans no nada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=3)\n",
    "cluster_2d = pca.fit_transform(cluster)\n",
    "\n",
    "get_centers = pca.transform(kmeans.cluster_centers_)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "for i in range(0,3,1):\n",
    "    plt.scatter(cluster_2d[marketing_feats[\"Cluster\"] == i, 0], cluster_2d[marketing_feats[\"Cluster\"] == i, 1], label=f\"Cluster{i}\")\n",
    "plt.scatter(get_centers[:, 0], get_centers[:, 1], marker='o', color='black', label=\"Cluster Center\") \n",
    "# DEUS DEIXA EU DIGITAR POR FAVOR PARA DE DAR ROLLBACK SERVIDOR MALDITO\n",
    "\n",
    "plt.title('Marketing Cluster')\n",
    "plt.xlabel('Component 1')\n",
    "plt.ylabel('Component 2')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "initial_df['Cluster'] = marketing_feats['Cluster']\n",
    "\n",
    "crosstab = pd.crosstab(initial_df['Cluster'], initial_df['Loyalty_Status'])\n",
    "\n",
    "display(crosstab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "initial_df['Cluster'] = marketing_feats['Cluster']\n",
    "\n",
    "crosstab = pd.crosstab(initial_df['Cluster'], initial_df['Response_to_Campaign'])\n",
    "\n",
    "display(crosstab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Não foi observada nenhuma relação entre as interações no _website_ com taxa de conversão ou até mesmo _status_ de lealdade do cliente.\n",
    "Muito possivelmente tais problemas vêm da base de dados balanceada, criada artificialmente, o que gera uma baixa correlação advinda da não-naturalidade dos dados. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "initial_df.drop(columns=['Products', 'Purchase_Date', 'Purchase_Date', 'Purchase_Amounts', 'Hashtags_Used', 'Price_Changes_Over_Time'], inplace=True)\n",
    "\n",
    "cols_to_encode = ['Gender', 'Age_Group', 'Loyalty_Status', 'Income_Level', 'Education_Level', 'Occupation', 'Response_to_Campaign', 'Location']\n",
    "\n",
    "for col in cols_to_encode:\n",
    "    initial_df[col+'_encoded'] = le.fit_transform(initial_df[col])\n",
    "    \n",
    "initial_df.drop(columns=cols_to_encode, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "---\n",
    "## 4. Classificador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def preprocess(X_treino, X_teste):\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    #print(f\"Antes do escalonamento.\\nX_treino.shape: {X_treino.shape}\\nX_teste.shape: {X_teste.shape}\")\n",
    "    \n",
    "    X_treino_escalonado = scaler.fit_transform(X_treino)\n",
    "    X_teste_escalonado = scaler.transform(X_treino)\n",
    "\n",
    "    #print(f\"Depois do escalonamento.\\nX_treino_escalonado.shape: {X_treino_escalonado.shape}\\nX_teste_escalonado.shape: {X_teste_escalonado.shape}\")\n",
    "    \n",
    "    return X_treino_escalonado, X_teste_escalonado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def confusion_matrix_(y_treino, y_predito):\n",
    "    if y_treino.shape != y_predito.shape:\n",
    "        raise ValueError(f\"y_treino e y_predito tem tamanhos diferentes.\\ny_treino.shape {y_treino.shape} != y_predito.shape {y_predito.shape}\")\n",
    "\n",
    "    n_classes = len(set(y_treino))\n",
    "\n",
    "    conf_matrix = np.zeros((n_classes, n_classes), dtype=int)\n",
    "\n",
    "    for i in range(len(y_treino)):\n",
    "        true_teste = y_treino[i]\n",
    "        true_preduto = y_predito[i]\n",
    "        conf_matrix[true_teste, true_preduto] += 1\n",
    "\n",
    "    return conf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Implementar as outras metricas pra facilitar\n",
    "\n",
    "def acuracia(conf_mat):\n",
    "    total = conf_mat.sum()\n",
    "    correct = np.trace(conf_mat)\n",
    "    return correct / total\n",
    "\n",
    "def precisao(conf_mat, classe):\n",
    "    TP = conf_mat[classe, classe]\n",
    "    FP = conf_mat.sum(axis=0)[classe] - TP\n",
    "    return TP / (TP + FP)\n",
    "\n",
    "def recall(conf_mat, classe):\n",
    "    TP = conf_mat[classe, classe]\n",
    "    FN = conf_mat.sum(axis=1)[classe] - TP\n",
    "    return TP / (TP + FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Mudar o nome e os parâmetros da função de acordo com sua métrica 1.\n",
    "def metric1_f1_score(conf_mat, classe):\n",
    "    prec = precisao(conf_mat, classe)\n",
    "    rec = recall(conf_mat, classe)\n",
    "    return 2 * prec * rec / (prec + rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Mudar o nome e os parâmetros da função de acordo com sua métrica 2.\n",
    "def metric2_especificidade(conf_mat, classe):\n",
    "  TN = conf_mat.sum(axis=0)[classe] - conf_mat[classe, classe]\n",
    "  FN = conf_mat.sum(axis=1)[classe] - conf_mat[classe, classe]\n",
    "  return TN / (TN + FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC  # Support Vector Machine\n",
    "from sklearn.neural_network import MLPClassifier  # Multi-Layer Perceptron\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, mean_squared_error, f1_score, recall_score,\n",
    "    precision_score, accuracy_score, roc_auc_score, log_loss, roc_curve\n",
    ")\n",
    "\n",
    "def classificacao(data, columns, target, preproc_fn, folds=5, plot=True):\n",
    "    \"\"\"\n",
    "    Executa classificação do conjunto de dados passado\n",
    "    ---------------------------------------------------------------\n",
    "    data:       DataFrame. Conjunto de dados\n",
    "    columns:    Lista de inteiros. Índice das colunas utilizadas no treinamento e teste\n",
    "    target:     Inteiro. Índice da coluna alvo\n",
    "    preproc_fn: Função. Faz o pré-processamento da base já separada em treino e teste\n",
    "    folds:      Inteiro. Número de folds na validação cruzada\n",
    "    plot:       Booleano. True para plotar os gráficos False para não plotar\n",
    "    ---------------------------------------------------------------\n",
    "    Realiza a classificação em vários modelos\n",
    "    Plota o gráfico de desempenho para cada classificador.\n",
    "    Retorna um dicionário com os classificadores treinados, as medidas de desempenho e matriz de confusão\n",
    "    \"\"\"\n",
    "    # Inicializa os modelos com os parâmetros solicitados\n",
    "    knn = KNeighborsClassifier(n_neighbors=3)\n",
    "    dt = DecisionTreeClassifier(criterion='gini', splitter='best', min_samples_split=int(len(data)*0.1))\n",
    "    lr = LogisticRegression(solver='lbfgs', max_iter=1000, n_jobs=-1)\n",
    "    gnb = GaussianNB()\n",
    "    rf = RandomForestClassifier(n_estimators=100, random_state=14)\n",
    "    svm = SVC(kernel='linear', probability=True)\n",
    "    mlp = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(100,), random_state=1)\n",
    "\n",
    "    clfs = [knn, dt, lr, gnb, rf, svm, mlp]\n",
    "    clfs_names = ['knn', 'dt', 'lr', 'gnb', 'rf', 'svm', 'mlp']\n",
    "    \n",
    "    kf = KFold(n_splits=folds, shuffle=True, random_state=14)\n",
    "    \n",
    "    results = {name: [] for name in clfs_names}\n",
    "    metrics = {\n",
    "        'Confusion Matrix': confusion_matrix,\n",
    "        'MSE': mean_squared_error,\n",
    "        'F1 Score': f1_score,\n",
    "        'Specificity': lambda y_true, y_pred: recall_score(y_true, y_pred, pos_label=0),\n",
    "        'Recall': recall_score,\n",
    "        'Precision': precision_score,\n",
    "        'Accuracy': accuracy_score,\n",
    "        'AUC': roc_auc_score,\n",
    "        'Log Loss': log_loss\n",
    "    }\n",
    "\n",
    "    for c, c_name in zip(clfs, clfs_names):\n",
    "        for train_index, test_index in kf.split(data[columns]):\n",
    "            x_train, x_test = data[columns].iloc[train_index], data[columns].iloc[test_index]\n",
    "            y_train, y_test = data.loc[train_index, target], data.loc[test_index, target]\n",
    "\n",
    "            X_treino_escalonado, X_teste_escalonado = preproc_fn(x_train, x_test)\n",
    "\n",
    "            clf = c.fit(X=X_treino_escalonado, y=y_train)\n",
    "            y_pred = clf.predict(X_teste_escalonado)\n",
    "            y_proba = clf.predict_proba(X_teste_escalonado)[:, 1] if hasattr(clf, \"predict_proba\") else None\n",
    "            \n",
    "            for metric_name, metric_fn in metrics.items():\n",
    "                if metric_name == 'AUC' and y_proba is None:\n",
    "                    results[c_name].append(np.nan)\n",
    "                elif metric_name == 'AUC':\n",
    "                    results[c_name].append(metric_fn(y_test, y_proba))\n",
    "                else:\n",
    "                    results[c_name].append(metric_fn(y_test, y_pred))\n",
    "            \n",
    "            # Plot ROC Curve\n",
    "            if plot and metric_name == 'AUC' and y_proba is not None:\n",
    "                fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
    "                plt.plot(fpr, tpr, label=f'{c_name} (AUC = {roc_auc_score(y_test, y_proba):.2f})')\n",
    "\n",
    "    if plot:\n",
    "        plt.figure(figsize=(15, 10))\n",
    "        for metric_name in metrics.keys():\n",
    "            if metric_name != 'Confusion Matrix':\n",
    "                plt.bar(range(len(clfs_names)), [mean(results[name][i]) for name in clfs_names], yerr=[std(results[name][i]) for name in clfs_names])\n",
    "                plt.xticks(range(len(clfs_names)), clfs_names, rotation=45)\n",
    "                plt.title(f'Desempenho dos classificadores - {metric_name}')\n",
    "                plt.show()\n",
    "        \n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('ROC Curve')\n",
    "        plt.legend(loc='lower right')\n",
    "        plt.show()\n",
    "    \n",
    "    return {'results': results, 'clfs': clfs}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "cols_feat = []\n",
    "\n",
    "#print(df.columns)\n",
    "\n",
    "for col in df.columns:\n",
    "    if col != \"Response_to_Campaign_encoded\":\n",
    "        cols_feat.append(col)\n",
    "\n",
    "print(\"10-fold Cross-Validation:\")\n",
    "print(classificacao(data=initial_df, columns=cols_feat, target='Response_to_Campaign_encoded', preproc_fn=preprocess, score_fn=metric1_f1_score, score_name=\"F1-Score\",fn_conf_matrix=confusion_matrix_, folds=10, plot=True))\n",
    "\n",
    "print(\"Leave-One-Out Cross-Validation:\")\n",
    "print(classificacao(data=initial_df, columns=cols_feat, target='Response_to_Campaign_encoded', preproc_fn=preprocess, score_fn=metric2_especificidade, score_name=\"Especificidade\", fn_conf_matrix=confusion_matrix_, folds=len(initial_df), plot=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
